{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7AaTiTh_ZJ2"
      },
      "source": [
        "# **A Hybrid LSTM-CNN-Attention Deep Learning Framework with Multi-Method Feature Selection and Sentiment Integration for Robust Stock Price Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "B4xUSJ9ShDS6",
        "outputId": "741351eb-fa24-4ff9-9b8a-27aa7433cf66"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 0. Setup: Kaggle API, Install/Import Libraries, Set Device\n",
        "# ==============================================================\n",
        "\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()  # Upload your kaggle.json\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "!kaggle datasets download -d franciscofeng/augmented-china-stock-data-with-fundamentals\n",
        "!unzip -q augmented-china-stock-data-with-fundamentals.zip\n",
        "\n",
        "\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "# Check GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wxERsbndtdzm",
        "outputId": "ab4889aa-aec6-45cd-8179-42c498e786b4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Input, LSTM, Conv1D, Dense, Dropout, concatenate\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "# ========================================================\n",
        "# 1. Imports and Setup\n",
        "# ========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LassoCV, LinearRegression\n",
        "from sklearn.feature_selection import RFE, mutual_info_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Conv1D, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ========================================================\n",
        "# 2. Data Loading\n",
        "# ========================================================\n",
        "# Change filenames as needed\n",
        "df_main = pd.read_csv('/content/stock_data.csv')\n",
        "df_company = pd.read_csv('/content/ticker_info.csv')\n",
        "\n",
        "# Merge company names\n",
        "df_main = df_main.merge(df_company[['ticker', 'company_name']], on='ticker', how='left')\n",
        "\n",
        "# ========================================================\n",
        "# 3. Sentiment Integration (replace with real if available)\n",
        "# ========================================================\n",
        "if 'sentiment' not in df_main.columns:\n",
        "    np.random.seed(SEED)\n",
        "    df_main['sentiment'] = np.random.normal(0, 1, len(df_main))\n",
        "\n",
        "# ========================================================\n",
        "# 4. Feature Engineering & Multi-Method Feature Selection\n",
        "# ========================================================\n",
        "feature_cols = [\n",
        "    'open', 'high', 'low', 'volume', 'outstanding_share', 'turnover', 'pe',\n",
        "    'pe_ttm', 'pb', 'ps', 'ps_ttm', 'dv_ratio', 'dv_ttm', 'total_mv', 'qfq_factor'\n",
        "]\n",
        "target_col = 'close'\n",
        "\n",
        "df_main['date'] = pd.to_datetime(df_main['date'])\n",
        "df_main = df_main.sort_values(['ticker', 'date'])\n",
        "df_main['next_close'] = df_main.groupby('ticker')[target_col].shift(-1)\n",
        "df_main = df_main.dropna(subset=feature_cols + ['next_close'])\n",
        "df_main[feature_cols] = df_main[feature_cols].fillna(df_main[feature_cols].median())\n",
        "\n",
        "# Sampling for feature selection (memory safe)\n",
        "scaler = StandardScaler()\n",
        "sample_size = min(100000, len(df_main))\n",
        "X_sample = df_main[feature_cols].sample(sample_size, random_state=SEED)\n",
        "y_sample = df_main.loc[X_sample.index, 'next_close']\n",
        "X_scaled = scaler.fit_transform(X_sample)\n",
        "X_s = pd.DataFrame(X_scaled, columns=feature_cols)\n",
        "\n",
        "# (a) Pearson Correlation\n",
        "pearson_scores = np.abs([np.corrcoef(X_s[c], y_sample)[0, 1] for c in feature_cols])\n",
        "# (b) Mutual Info\n",
        "mic_scores = mutual_info_regression(X_s, y_sample, random_state=SEED)\n",
        "# (c) Lasso\n",
        "lasso = LassoCV(cv=3, random_state=SEED, n_jobs=-1, max_iter=10000)\n",
        "lasso.fit(X_s, y_sample)\n",
        "lasso_scores = np.abs(lasso.coef_)\n",
        "# (d) RFE\n",
        "lr = LinearRegression()\n",
        "rfe = RFE(lr, n_features_to_select=1)\n",
        "rfe.fit(X_s, y_sample)\n",
        "rfe_scores = -rfe.ranking_\n",
        "# (e) Null Importance\n",
        "def get_null_importance(X, y, base_model, n_runs=5):\n",
        "    base_model.fit(X, y)\n",
        "    real_importance = np.abs(base_model.coef_) if hasattr(base_model, \"coef_\") else np.abs(base_model.feature_importances_)\n",
        "    null_imp = np.zeros((n_runs, len(feature_cols)))\n",
        "    for i in range(n_runs):\n",
        "        y_shuffled = shuffle(y, random_state=i)\n",
        "        base_model.fit(X, y_shuffled)\n",
        "        this_imp = np.abs(base_model.coef_) if hasattr(base_model, \"coef_\") else np.abs(base_model.feature_importances_)\n",
        "        null_imp[i] = this_imp\n",
        "    return real_importance - null_imp.mean(axis=0)\n",
        "null_scores = get_null_importance(X_s, y_sample, LinearRegression(), n_runs=5)\n",
        "\n",
        "score_df = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'pearson': pearson_scores,\n",
        "    'mic': mic_scores,\n",
        "    'lasso': lasso_scores,\n",
        "    'rfe': rfe_scores,\n",
        "    'null_importance': null_scores\n",
        "})\n",
        "def quartile_rank(series):\n",
        "    q1, q2 = series.quantile(1/3), series.quantile(2/3)\n",
        "    return series.apply(lambda v: 'Strong' if v > q2 else ('Medium' if v > q1 else 'Weak'))\n",
        "for col in ['pearson', 'mic', 'lasso', 'rfe', 'null_importance']:\n",
        "    score_df[f'{col}_rank'] = quartile_rank(score_df[col])\n",
        "score_df['num_strong_or_medium'] = score_df[\n",
        "    [f'{col}_rank' for col in ['pearson', 'mic', 'lasso', 'rfe', 'null_importance']]\n",
        "].apply(lambda row: sum(r in ['Strong', 'Medium'] for r in row), axis=1)\n",
        "selected_features = score_df[score_df['num_strong_or_medium'] >= 4]['feature'].tolist()\n",
        "if 'sentiment' in df_main.columns and 'sentiment' not in selected_features:\n",
        "    selected_features.append('sentiment')\n",
        "print(\"Selected features for modeling:\", selected_features)\n",
        "#display(score_df) # Uncomment if in Jupyter\n",
        "\n",
        "# ========================================================\n",
        "# 5. Final Data Prep for Modeling (fill missing, etc)\n",
        "# ========================================================\n",
        "df_main[selected_features] = df_main[selected_features].fillna(df_main[selected_features].median())\n",
        "# (Target already present as 'next_close')\n",
        "\n",
        "# ========================================================\n",
        "# 6. Windowed Time-Series Construction\n",
        "# ========================================================\n",
        "def create_windowed_dataset(df, selected_features, target_col='next_close', window_size=5):\n",
        "    X, y, meta = [], [], []\n",
        "    for ticker in df['ticker'].unique():\n",
        "        sub = df[df['ticker'] == ticker]\n",
        "        arr = sub[selected_features].values\n",
        "        targets = sub[target_col].values\n",
        "        names = sub['company_name'].values\n",
        "        for i in range(len(arr) - window_size):\n",
        "            X.append(arr[i:i+window_size])\n",
        "            y.append(targets[i+window_size])\n",
        "            meta.append(names[i+window_size])\n",
        "    return np.array(X), np.array(y), np.array(meta)\n",
        "window_size = 5\n",
        "X_windowed, y_windowed, meta_windowed = create_windowed_dataset(df_main, selected_features, window_size=window_size)\n",
        "print(\"Windowed data shape:\", X_windowed.shape, y_windowed.shape)\n",
        "\n",
        "# ========================================================\n",
        "# 7. Scaling (fit on all windowed features)\n",
        "# ========================================================\n",
        "n_samples, n_window, n_feat = X_windowed.shape\n",
        "X_reshaped = X_windowed.reshape(-1, n_feat)\n",
        "scaler_final = StandardScaler()\n",
        "X_scaled = scaler_final.fit_transform(X_reshaped)\n",
        "X_windowed_scaled = X_scaled.reshape(n_samples, n_window, n_feat)\n",
        "\n",
        "# ========================================================\n",
        "# 8. Train/Test Split\n",
        "# ========================================================\n",
        "X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n",
        "    X_windowed_scaled, y_windowed, meta_windowed, test_size=0.05, random_state=SEED\n",
        ")\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "\n",
        "# ========================================================\n",
        "# 9. Hybrid LSTM-CNN-Attention Model\n",
        "# ========================================================\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='normal')\n",
        "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1), initializer='zeros')\n",
        "        super(Attention, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        e = tf.keras.backend.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
        "        a = tf.keras.backend.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return tf.keras.backend.sum(output, axis=1)\n",
        "input_shape = X_train.shape[1:]\n",
        "inp = Input(shape=input_shape)\n",
        "x1 = LSTM(64, return_sequences=True)(inp)\n",
        "x1 = Dropout(0.2)(x1)\n",
        "x2 = Conv1D(64, kernel_size=1, activation='relu', padding='same')(inp)\n",
        "x2 = Dropout(0.2)(x2)\n",
        "x = concatenate([x1, x2])\n",
        "att_out = Attention()(x)\n",
        "dense = Dense(32, activation='relu')(att_out)\n",
        "dense = Dropout(0.1)(dense)\n",
        "out = Dense(1)(dense)\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.summary()\n",
        "\n",
        "# ========================================================\n",
        "# 10. Training with Early Stopping\n",
        "# ========================================================\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=50,\n",
        "    batch_size=2048,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# ========================================================\n",
        "# 11. Evaluation and Insights\n",
        "# ========================================================\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Test MAE: {mae:.4f}\")\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "print(f\"Test R^2: {r2:.4f}\")\n",
        "\n",
        "print(\"Sample predictions (Actual vs Predicted, Company):\")\n",
        "for i in range(5):\n",
        "    print(f\"Company: {meta_test[i]}, Actual: {y_test[i]:.2f}, Predicted: {y_pred[i]:.2f}, Error: {y_test[i]-y_pred[i]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZfXw_Nt3qvd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
